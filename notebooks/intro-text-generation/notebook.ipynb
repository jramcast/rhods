{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf6813a-573a-4adb-8dd4-63c9adb000a3",
   "metadata": {},
   "source": [
    "# Text Generation Demo\n",
    "\n",
    "This demo shows you how to fine-tune a pretrained language model for text generation by using _Causal language modeling_.\n",
    "Fine-tuning is the process of training a pretrained AI model on a specific task or dataset to adapt the model to your needs.\n",
    "In this way, you refine the performance of the model for your specific use case, without having to retrain the model from scratch.\n",
    "\n",
    "Causal language modeling is a natural language processing technique that predicts the next token of a sequence of tokens, and it's typically used for text generation.\n",
    "Large language models (LLMs) such as Llama2 and GPT-4 have shown splendid results in text generation.\n",
    "However, the size of this models require vasts amounts of computational and memory resources for training, and even for fine-tuning the pretrained models.\n",
    "\n",
    "Instead, this demo fine tunes the DistilGPT-2 model, which is a smaller model developed by Hugging Face.\n",
    "Note that, altough the model is smaller, the fine-tuning step still might take hours if you do not have access to a GPU.\n",
    "For that reason, this notebook deactivates the training phase by default, and instead provides the final fine tuned model.\n",
    "If you wish to change this behaviour, change the following variable to `True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "95ec0471-1955-4d64-8a47-2dafc31ed473",
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_TRAIN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61cbd4a-ba0f-4fc3-ad7b-2ce660381861",
   "metadata": {},
   "source": [
    "First, install the dependencies that are required for this demo and are not installed in the PyTorch workbench.\n",
    "This demo uses the `transformers` library ecosystem, which is a common choice when training language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6e19bc14-6e1d-4238-b321-400fc97d081d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch]==4.34.0 in /opt/app-root/lib/python3.9/site-packages (4.34.0)\n",
      "Requirement already satisfied: datasets==2.14.5 in /opt/app-root/lib/python3.9/site-packages (2.14.5)\n",
      "Requirement already satisfied: evaluate==0.4.0 in /opt/app-root/lib/python3.9/site-packages (0.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib/python3.9/site-packages (from transformers[torch]==4.34.0) (23.1)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from transformers[torch]==4.34.0) (2.31.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/app-root/lib/python3.9/site-packages (from transformers[torch]==4.34.0) (1.24.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from transformers[torch]==4.34.0) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/app-root/lib/python3.9/site-packages (from transformers[torch]==4.34.0) (4.66.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/app-root/lib/python3.9/site-packages (from transformers[torch]==4.34.0) (0.16.4)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /opt/app-root/lib/python3.9/site-packages (from transformers[torch]==4.34.0) (0.14.0)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from transformers[torch]==4.34.0) (3.12.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/app-root/lib/python3.9/site-packages (from transformers[torch]==4.34.0) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/app-root/lib/python3.9/site-packages (from transformers[torch]==4.34.0) (0.3.3)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.10 in /opt/app-root/lib/python3.9/site-packages (from transformers[torch]==4.34.0) (1.13.1)\n",
      "Requirement already satisfied: accelerate>=0.20.3 in /opt/app-root/lib/python3.9/site-packages (from transformers[torch]==4.34.0) (0.23.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.5) (0.3.7)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.5) (13.0.0)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.5) (2023.6.0)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.5) (1.5.3)\n",
      "Requirement already satisfied: multiprocess in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.5) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.5) (3.8.5)\n",
      "Requirement already satisfied: xxhash in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.5) (3.4.1)\n",
      "Requirement already satisfied: responses<0.19 in /opt/app-root/lib/python3.9/site-packages (from evaluate==0.4.0) (0.18.0)\n",
      "Requirement already satisfied: psutil in /opt/app-root/lib/python3.9/site-packages (from accelerate>=0.20.3->transformers[torch]==4.34.0) (5.9.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.5) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.5) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.5) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.5) (1.9.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.5) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.5) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.5) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]==4.34.0) (4.7.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers[torch]==4.34.0) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers[torch]==4.34.0) (1.26.16)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers[torch]==4.34.0) (3.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/app-root/lib/python3.9/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]==4.34.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/app-root/lib/python3.9/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]==4.34.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/app-root/lib/python3.9/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]==4.34.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/app-root/lib/python3.9/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]==4.34.0) (8.5.0.96)\n",
      "Requirement already satisfied: setuptools in /opt/app-root/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch!=1.12.0,>=1.10->transformers[torch]==4.34.0) (68.1.0)\n",
      "Requirement already satisfied: wheel in /opt/app-root/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch!=1.12.0,>=1.10->transformers[torch]==4.34.0) (0.38.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets==2.14.5) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets==2.14.5) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets==2.14.5) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers[torch]==4.34.0 datasets==2.14.5 evaluate==0.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9c0b8e-e45a-4c2f-b640-276b3c1bee90",
   "metadata": {},
   "source": [
    "Import the dependencies for the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a9dfbc87-cb5b-46a0-aecb-4df54c5108a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    pipeline, AutoTokenizer, DataCollatorForLanguageModeling, \n",
    "    AutoModelForCausalLM, TrainingArguments, Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f00e40-7e2f-475d-be42-6b4dcf091ff4",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "The demo fine-tunes the DistilGPT-2 model to better generate text related to Open Data Hub.\n",
    "To this end, the demo provides a subset of the asciidoc source code of the Open Data Hub Documentation in the `odh-merged-docs.adoc` file.\n",
    "The complete documentation is available at https://github.com/opendatahub-io/opendatahub-documentation.\n",
    "\n",
    "You can use this data to _teach_ the model how to write more Open Data Hub content.\n",
    "\n",
    "Load the data with the `datasets` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6f2e88c9-8c48-4405-84cf-d98780e2a9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 3793\n",
       "})"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"text\", data_files={\"data\": \"odh-merged-docs.adoc\"}, split=\"data\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b1979-8b7f-412a-97dc-e4506af156b5",
   "metadata": {},
   "source": [
    "## Create the Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805537f3-5be2-4084-a567-d82f2c225b4a",
   "metadata": {},
   "source": [
    "Create a tokenizer.\n",
    "A tokenizer is a key component of language models.\n",
    "It converts raw text into numerical ids (tokens) that can be processed by the neural network inside the model.\n",
    "\n",
    "In this case, use the the tokenizer that is specific for the DistilGPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ead14f84-62f4-4aff-a51e-bc96486a32af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658512c7-f2b9-45ba-b461-5e8ba2617434",
   "metadata": {},
   "source": [
    "You can test the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ddfbd995-02eb-4de8-8d32-8ca08208b837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [15496, 995, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d7a89-6714-4fe1-8510-d389eb653f33",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Preprocess the data by tokenizing the text and grouping the samples in batches.\n",
    "You must also divide the data into training and testing splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "bd192497-cfaa-46a3-bbba-a30f6f947171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820519ee9c9e4fe38b66f8cfbe80acb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3034 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7229 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1070 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3136 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1156 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce9a933d41d4cc5b2a120cf77da0d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/759 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2149 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35372 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8173 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2031 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(samples):\n",
    "    return tokenizer([f\"{x}\\n\".join(x) for x in samples[\"text\"]])\n",
    "\n",
    "ds = ds.train_test_split(test_size=0.2)\n",
    "\n",
    "tokenized_ds = ds.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=ds[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f265f80-7124-4e47-8a06-5209e82de15f",
   "metadata": {},
   "source": [
    "Inspect the dataset and verify that two subsets are included now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5b6b9af6-34cf-4824-a083-bc20c5088473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 3034\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 759\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064d3d58-1027-44e9-a0cb-96bf601eaa27",
   "metadata": {},
   "source": [
    "Concatenate all the token sequences and chunk them into blocks.\n",
    "\n",
    "This is important to ensure that every block of tokens that we use for training fits in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "7c4c9aa6-c248-4f7e-bd68-a75ba85811e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f2c2944e4e44f18a0b738d03f819a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3034 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad104a8e1fd41e0b8778b9ae4844cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/759 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21529\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 256\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported \n",
    "    # it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized_ds.map(group_texts, batched=True, num_proc=4)\n",
    "lm_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab21f966-de33-420b-9271-b89f1d5ceb34",
   "metadata": {},
   "source": [
    "Finally, define the data collation and the padding strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b670cd10-17b6-4e7c-8943-a1452534a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595b03c8-fc1c-415b-94db-64b7589abb9c",
   "metadata": {},
   "source": [
    "## Training (fine-tuning)\n",
    "\n",
    "Load the pretrained base DistilGPT-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "779e0dda-579e-4885-96e1-73db8ba87d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7a6356-7f23-4b26-aeca-f47eb74697a0",
   "metadata": {},
   "source": [
    "Train the model with your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "391cd1ff-f4d2-465c-9982-e51a02da89be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_TRAIN:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"my_model\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=lm_dataset[\"train\"],\n",
    "        eval_dataset=lm_dataset[\"test\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fad29b-f417-432a-8966-bcc7e244b5e4",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9475809a-ffff-4d58-bd9c-1ad2803fbaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_TRAIN:\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed63f90-e63b-4ad8-bdc4-68ad64062bfb",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Generate text given the following prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a4ae53d3-541f-4cb5-9eff-debe08fc00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Use Elyra to\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01518676-29a6-449f-85e4-590754ec68b1",
   "metadata": {},
   "source": [
    "First, verify the text produced by the base DistilGPT-2 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2f24172b-070f-4e89-9247-215ce1db11d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Elyra to keep her head in head for the rest of the night and be able to walk without making any mistakes.\n",
      "\n",
      "Feyman also uses a sword to get the rest of the week and when the fight starts, she will let\n"
     ]
    }
   ],
   "source": [
    "base_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "print(base_generator(prompt)[0][\"generated_text\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e9f63d-d2a3-4959-8e72-c9d5a281df1b",
   "metadata": {},
   "source": [
    "Now, test the text generated by the fine tuned model.\n",
    "The output might sound closer to the OpenDataHub docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "277a403e-f652-4be9-a2f5-60b8ef602c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Elyra to enable you to test and implement them into intelligent applications. | Elyra Elyra is an extension for Jupyter and provides you with a Pipeline Editor to create pipeline workflows that can be executed in {productname-short\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"./my_model\", tokenizer=tokenizer)\n",
    "print(generator(prompt)[0][\"generated_text\"].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
